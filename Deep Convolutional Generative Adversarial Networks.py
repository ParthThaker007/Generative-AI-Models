import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam

# Load MNIST dataset
(x_train, _), (_, _) = mnist.load_data()

# Preprocess the data
x_train = x_train.astype('float32') / 255.
x_train = np.expand_dims(x_train, axis=-1)

# Define the generator model
latent_dim = 100

generator_inputs = Input(shape=(latent_dim,))
x = Dense(7 * 7 * 128)(generator_inputs)
x = Reshape((7, 7, 128))(x)
x = Conv2DTranspose(64, 4, strides=2, padding='same')(x)
x = LeakyReLU(alpha=0.2)(x)
x = Conv2DTranspose(1, 4, strides=2, padding='same', activation='sigmoid')(x)
generator_outputs = x

generator = Model(generator_inputs, generator_outputs)

# Define the discriminator model
discriminator_inputs = Input(shape=(28, 28, 1))
x = Conv2D(64, 4, strides=2, padding='same')(discriminator_inputs)
x = LeakyReLU(alpha=0.2)(x)
x = Conv2D(128, 4, strides=2, padding='same')(x)
x = LeakyReLU(alpha=0.2)(x)
x = Flatten()(x)
x = Dense(1, activation='sigmoid')(x)
discriminator_outputs = x

discriminator = Model(discriminator_inputs, discriminator_outputs)

# Compile the discriminator
discriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')

# Combine the generator and discriminator into a GAN model
gan_inputs = Input(shape=(latent_dim,))
gan_outputs = discriminator(generator(gan_inputs))
gan = Model(gan_inputs, gan_outputs)

# Compile the GAN model
gan.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')

# Training loop
batch_size = 1
epochs = 1000
sample_interval = 10

# Create the output directory if it doesn't exist
output_dir = 'gan_output'
os.makedirs(output_dir, exist_ok=True)

for epoch in range(epochs):
    # Generate random noise as input to the generator
    noise = np.random.normal(0, 1, (batch_size, latent_dim))

    # Generate fake images using the generator
    generated_images = generator.predict(noise)

    # Select a random batch of real images from the dataset
    real_images = x_train[np.random.randint(0, x_train.shape[0], batch_size)]

    # Concatenate the real and fake images
    combined_images = np.concatenate([real_images, generated_images])

    # Create the labels for the discriminator (0 for real images, 1 for fake images)
    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])

    # Add noise to the labels for regularization
    labels += 0.05 * np.random.random(labels.shape)

    # Train the discriminator
    d_loss = discriminator.train_on_batch(combined_images, labels)

    # Generate new random noise as input to the generator
    noise = np.random.normal(0, 1, (batch_size, latent_dim))

    # Create labels for the generator (1 for all fake images)
    misleading_targets = np.ones((batch_size, 1))

    # Train the generator (via the GAN model) using the misleading targets
    g_loss = gan.train_on_batch(noise, misleading_targets)

    # Print the progress
    if epoch % sample_interval == 0:
        print(f"Epoch {epoch}/{epochs} | Discriminator loss: {d_loss} | Generator loss: {g_loss}")

        # Save sample images generated by the generator
        img = tf.keras.preprocessing.image.array_to_img(generated_images[0] * 255., scale=False)
        img.save(f"{output_dir}/sample_{epoch}.png")
